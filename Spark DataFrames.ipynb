{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames \"Graded\" Assignment Ruben Tak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Start a simple Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-03T16:41:19.100379Z",
     "end_time": "2023-05-03T16:41:19.105436Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('my_app').getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-03T16:41:19.370045Z",
     "end_time": "2023-05-03T16:41:19.379275Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. (0.5p) Load the walmart_stock.csv and let Spark infer the data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-03T16:41:19.825676Z",
     "end_time": "2023-05-03T16:41:20.884388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|  Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "|2012-01-03|         59.970001|         61.060001|         59.869999|         60.330002|12668800|52.619234999999996|\n",
      "|2012-01-04|60.209998999999996|         60.349998|         59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "|2012-01-05|         59.349998|         59.619999|         58.369999|         59.419998|12768200|         51.825539|\n",
      "|2012-01-06|         59.419998|         59.450001|         58.869999|              59.0| 8069400|          51.45922|\n",
      "|2012-01-09|         59.029999|         59.549999|         58.919998|             59.18| 6679300|51.616215000000004|\n",
      "|2012-01-10|             59.43|59.709998999999996|             58.98|59.040001000000004| 6907300|         51.494109|\n",
      "|2012-01-11|         59.060001|         59.529999|59.040001000000004|         59.400002| 6365600|         51.808098|\n",
      "|2012-01-12|59.790001000000004|              60.0|         59.400002|              59.5| 7236400|51.895315999999994|\n",
      "|2012-01-13|             59.18|59.610001000000004|59.009997999999996|59.540001000000004| 7729300|51.930203999999996|\n",
      "|2012-01-17|         59.869999|60.110001000000004|             59.52|         59.849998| 8500000|         52.200581|\n",
      "|2012-01-18|59.790001000000004|         60.029999|         59.650002|60.009997999999996| 5911400|         52.340131|\n",
      "|2012-01-19|             59.93|             60.73|             59.75|60.610001000000004| 9234600|         52.863447|\n",
      "|2012-01-20|             60.75|             61.25|         60.669998|61.009997999999996|10378800|53.212320999999996|\n",
      "|2012-01-23|         60.810001|             60.98|60.509997999999996|             60.91| 7134100|         53.125104|\n",
      "|2012-01-24|             60.75|              62.0|             60.75|61.389998999999996| 7362800| 53.54375400000001|\n",
      "|2012-01-25|             61.18|61.610001000000004|61.040001000000004|         61.470001| 5915800| 53.61353100000001|\n",
      "|2012-01-26|         61.799999|             61.84|             60.77|         60.970001| 7436200|         53.177436|\n",
      "|2012-01-27|60.860001000000004|         61.119999|60.540001000000004|60.709998999999996| 6287300|         52.950665|\n",
      "|2012-01-30|         60.470001|             61.32|         60.349998|         61.299999| 7636900|53.465256999999994|\n",
      "|2012-01-31|         61.529999|             61.57|         60.580002|61.360001000000004| 9761500|53.517590000000006|\n",
      "+----------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('4.1. walmart_stock.csv', header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. (0.25p) Display the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-05-03T16:41:20.885140Z",
     "end_time": "2023-05-03T16:41:20.893686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. (0.25p) Display the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-05-03T16:41:21.073105Z",
     "end_time": "2023-05-03T16:41:21.078435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (0.25p) Display the first 3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-05-03T16:41:21.570534Z",
     "end_time": "2023-05-03T16:41:21.687517Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[Row(Date=datetime.date(2012, 1, 3), Open=59.970001, High=61.060001, Low=59.869999, Close=60.330002, Volume=12668800, Adj Close=52.619234999999996),\n Row(Date=datetime.date(2012, 1, 4), Open=60.209998999999996, High=60.349998, Low=59.470001, Close=59.709998999999996, Volume=9593300, Adj Close=52.078475),\n Row(Date=datetime.date(2012, 1, 5), Open=59.349998, High=59.619999, Low=58.369999, Close=59.419998, Volume=12768200, Adj Close=51.825539)]"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use describe() to learn about the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-05-03T16:41:22.098345Z",
     "end_time": "2023-05-03T16:41:22.943176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|summary|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|  count|              1258|             1258|             1258|             1258|             1258|             1258|\n",
      "|   mean| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
      "| stddev|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
      "|    min|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
      "|    max|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. (1p) Create a new dataframe with a column called HV Ratio that is the ratio of the High Price versus volume of stock traded for a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-05-03T16:41:22.943385Z",
     "end_time": "2023-05-03T16:41:23.091346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+--------+------------------+--------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|  Volume|         Adj Close|            HV Ratio|\n",
      "+----------+------------------+------------------+------------------+------------------+--------+------------------+--------------------+\n",
      "|2012-01-03|         59.970001|         61.060001|         59.869999|         60.330002|12668800|52.619234999999996|4.819714653321546E-6|\n",
      "|2012-01-04|60.209998999999996|         60.349998|         59.470001|59.709998999999996| 9593300|         52.078475|6.290848613094555E-6|\n",
      "|2012-01-05|         59.349998|         59.619999|         58.369999|         59.419998|12768200|         51.825539|4.669412994783916E-6|\n",
      "|2012-01-06|         59.419998|         59.450001|         58.869999|              59.0| 8069400|          51.45922|7.367338463826307E-6|\n",
      "|2012-01-09|         59.029999|         59.549999|         58.919998|             59.18| 6679300|51.616215000000004|8.915604778943901E-6|\n",
      "|2012-01-10|             59.43|59.709998999999996|             58.98|59.040001000000004| 6907300|         51.494109|8.644477436914568E-6|\n",
      "|2012-01-11|         59.060001|         59.529999|59.040001000000004|         59.400002| 6365600|         51.808098|9.351828421515645E-6|\n",
      "|2012-01-12|59.790001000000004|              60.0|         59.400002|              59.5| 7236400|51.895315999999994| 8.29141562102703E-6|\n",
      "|2012-01-13|             59.18|59.610001000000004|59.009997999999996|59.540001000000004| 7729300|51.930203999999996|7.712212102001476E-6|\n",
      "|2012-01-17|         59.869999|60.110001000000004|             59.52|         59.849998| 8500000|         52.200581|7.071764823529412E-6|\n",
      "|2012-01-18|59.790001000000004|         60.029999|         59.650002|60.009997999999996| 5911400|         52.340131|1.015495466386981E-5|\n",
      "|2012-01-19|             59.93|             60.73|             59.75|60.610001000000004| 9234600|         52.863447|6.576354146362592...|\n",
      "|2012-01-20|             60.75|             61.25|         60.669998|61.009997999999996|10378800|53.212320999999996| 5.90145296180676E-6|\n",
      "|2012-01-23|         60.810001|             60.98|60.509997999999996|             60.91| 7134100|         53.125104|8.547679455011844E-6|\n",
      "|2012-01-24|             60.75|              62.0|             60.75|61.389998999999996| 7362800| 53.54375400000001|8.420709512685392E-6|\n",
      "|2012-01-25|             61.18|61.610001000000004|61.040001000000004|         61.470001| 5915800| 53.61353100000001|1.041448341728929...|\n",
      "|2012-01-26|         61.799999|             61.84|             60.77|         60.970001| 7436200|         53.177436|8.316075414862431E-6|\n",
      "|2012-01-27|60.860001000000004|         61.119999|60.540001000000004|60.709998999999996| 6287300|         52.950665|9.721183814992126E-6|\n",
      "|2012-01-30|         60.470001|             61.32|         60.349998|         61.299999| 7636900|53.465256999999994|8.029436027707578E-6|\n",
      "|2012-01-31|         61.529999|             61.57|         60.580002|61.360001000000004| 9761500|53.517590000000006|6.307432259386365E-6|\n",
      "+----------+------------------+------------------+------------------+------------------+--------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataframe with a column called HV Ratio that is the ratio of the High Price versus volume of stock traded for a day\n",
    "df2 = df.withColumn('HV Ratio', df['High']/df['Volume'])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. (0.5p) Which day did we had the highest value in High?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-05-03T16:41:23.136071Z",
     "end_time": "2023-05-03T16:41:23.297262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "datetime.date(2015, 1, 13)"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.orderBy(df['High'].desc()).head(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. (0.5p) What is the mean of the High column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-05-03T16:41:23.609573Z",
     "end_time": "2023-05-03T16:41:23.992090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|        avg(High)|\n",
      "+-----------------+\n",
      "|72.83938807631165|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "df.select(mean('High')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. (0.5p) What is the max and min of the Volume column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-03T16:41:24.404106Z",
     "end_time": "2023-05-03T16:41:24.558175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|max(Volume)|min(Volume)|\n",
      "+-----------+-----------+\n",
      "|   80898100|    2094900|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, min\n",
    "df.select(max('Volume'), min('Volume')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. (1p) How many days was the Open lower than 60 dollars and the Close higher than 70?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-05-03T16:41:24.878848Z",
     "end_time": "2023-05-03T16:41:25.023114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter((df['Open'] < 60) & (df['Close'] > 70)).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. (1.5p) What percentage of the time was the Close greater than 70 dollars ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-05-03T16:41:25.383782Z",
     "end_time": "2023-05-03T16:41:25.669718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "68.44197138314784"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.filter(df['Close'] > 70).count() / df.count()) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. (1p) What is the Pearson correlation between Open and Close?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-05-03T16:41:26.181708Z",
     "end_time": "2023-05-03T16:41:26.499365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "| corr(Open, Close)|\n",
      "+------------------+\n",
      "|0.9958680008553993|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "df.select(corr('Open', 'Close')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. (1p) What is the max Open per year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-05-03T16:41:27.267570Z",
     "end_time": "2023-05-03T16:41:27.586330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|Year|        max(Open)|\n",
      "+----+-----------------+\n",
      "|2015|        90.800003|\n",
      "|2013|        81.209999|\n",
      "|2014|87.08000200000001|\n",
      "|2012|        77.599998|\n",
      "|2016|             74.5|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "df2 = df.withColumn('Year', year(df['Date']))\n",
    "df2.groupBy('Year').max('Open').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. (1.5p) What is the average High per month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-05-03T16:41:28.816756Z",
     "end_time": "2023-05-03T16:41:29.202181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|Month|        avg(High)|\n",
      "+-----+-----------------+\n",
      "|   12|73.35566025471698|\n",
      "|    1|71.97009924752473|\n",
      "|    6| 72.9262265471698|\n",
      "|    3|72.20289709345795|\n",
      "|    5|72.71783049056604|\n",
      "|    9|72.60294148039216|\n",
      "|    4|73.45123863809525|\n",
      "|    8|73.51781822727274|\n",
      "|    7|74.77878527102803|\n",
      "|   10|72.12963649090909|\n",
      "|   11|72.52673264356439|\n",
      "|    2|71.72845387628868|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import month\n",
    "df2 = df.withColumn('Month', month(df['Date']))\n",
    "df2.groupBy('Month').avg('High').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. (1p) Which is the month with the max average Low?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-03T16:42:47.241613Z",
     "end_time": "2023-05-03T16:42:47.575700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "7"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.groupBy('Month').avg('Low').orderBy('avg(Low)', ascending=False).head(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. (2p) Create the following pipeline:\n",
    "1. Filter all the rows with a Close greater than 60\n",
    "2. Create a new column called \"my_column\" computed by 2*Close\n",
    "3. Select \"Date\", \"my_column\" and \"Close\" columns\n",
    "4. Create a new column with the month of the year\n",
    "5. Compute the average \"my_column\" value by month\n",
    "6. Display the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "# Create the following pipeline:\n",
    "# 1. Filter all the rows with a Close greater than 60\n",
    "df2 = df.filter(df['Close'] > 60)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-03T16:43:04.989030Z",
     "end_time": "2023-05-03T16:43:05.006712Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "# 2. Create a new column called \"my_column\" computed by 2*Close\n",
    "df2 = df2.withColumn('my_column', df2['Close']*2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-03T16:43:06.052084Z",
     "end_time": "2023-05-03T16:43:06.067319Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "# 3. Select \"Date\", \"my_column\" and \"Close\" columns\n",
    "df2 = df2.select('Date', 'my_column', 'Close')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-03T16:43:06.436442Z",
     "end_time": "2023-05-03T16:43:06.450209Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "# 4. Create a new column with the month of the year\n",
    "from pyspark.sql.functions import month\n",
    "df2 = df2.withColumn('Month', month(df2['Date']))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-03T16:43:06.738640Z",
     "end_time": "2023-05-03T16:43:06.751021Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|Month|    avg(my_column)|\n",
      "+-----+------------------+\n",
      "|   12|149.17361757446807|\n",
      "|    1| 145.2523908478261|\n",
      "|    6| 144.9907548490566|\n",
      "|    3|145.03782142574255|\n",
      "|    5|147.99085102127665|\n",
      "|    9| 144.3682357058823|\n",
      "|    4|148.56604156250003|\n",
      "|    8|146.05963710909091|\n",
      "|    7|148.87943887850466|\n",
      "|   10|146.40081634693874|\n",
      "|   11|149.00418655813954|\n",
      "|    2|144.27164901098905|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Compute the average \"my_column\" value by month\n",
    "df2.groupBy('Month').avg('my_column').show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-03T16:43:07.113030Z",
     "end_time": "2023-05-03T16:43:07.349468Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16. (2p) Create a pipeline to solve the following issue\n",
    "Get the maximum Close value by week, month and year, excluding all the Mondays and Tuesdays from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-03T16:46:37.333610Z",
     "end_time": "2023-05-03T16:46:38.777942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|Week|       max(Close)|\n",
      "+----+-----------------+\n",
      "|   1|        86.790001|\n",
      "|   2|        90.470001|\n",
      "|   3|        89.309998|\n",
      "|   4|        88.510002|\n",
      "|   5|        87.720001|\n",
      "|   6|87.33000200000001|\n",
      "|   7|        87.290001|\n",
      "|   8|        86.290001|\n",
      "|   9|            84.57|\n",
      "|  10|            83.57|\n",
      "|  11|            82.07|\n",
      "|  12|        83.239998|\n",
      "|  13|        83.050003|\n",
      "|  14|            82.25|\n",
      "|  15|        81.029999|\n",
      "|  16|        80.150002|\n",
      "|  17|        79.839996|\n",
      "|  18|        79.709999|\n",
      "|  19|        79.199997|\n",
      "|  20|        79.860001|\n",
      "+----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+-----------------+\n",
      "|Month|       max(Close)|\n",
      "+-----+-----------------+\n",
      "|    1|        90.470001|\n",
      "|    2|87.33000200000001|\n",
      "|    3|            83.57|\n",
      "|    4|        81.029999|\n",
      "|    5|        79.860001|\n",
      "|    6|            77.32|\n",
      "|    7|        78.550003|\n",
      "|    8|            78.75|\n",
      "|    9|        77.510002|\n",
      "|   10|        78.290001|\n",
      "|   11|        87.540001|\n",
      "|   12|        86.910004|\n",
      "+-----+-----------------+\n",
      "\n",
      "+----+----------+\n",
      "|Year|max(Close)|\n",
      "+----+----------+\n",
      "|2012| 77.029999|\n",
      "|2013| 81.209999|\n",
      "|2014| 87.540001|\n",
      "|2015| 90.470001|\n",
      "|2016| 74.300003|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import weekofyear, year, month, dayofweek\n",
    "df2 = df.filter((dayofweek(df['Date']) != 1) & (dayofweek(df['Date']) != 2))\n",
    "df2 = df2.withColumn('Week', weekofyear(df2['Date']))\n",
    "df2 = df2.withColumn('Month', month(df2['Date']))\n",
    "df2 = df2.withColumn('Year', year(df2['Date']))\n",
    "df2.groupBy('Week').max('Close').orderBy('Week').show()\n",
    "df2.groupBy('Month').max('Close').orderBy('Month').show()\n",
    "df2.groupBy('Year').max('Close').orderBy('Year').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17. (0.5p) Compute the standat deviation of the High column and assign it an alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-03T11:48:59.469278Z",
     "end_time": "2023-05-03T11:49:00.903845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|              std|\n",
      "+-----------------+\n",
      "|6.768186808159218|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import stddev\n",
    "df.select(stddev('High').alias('std')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18. (0.5p) Display the average Close value with just 1 decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-03T16:49:57.567016Z",
     "end_time": "2023-05-03T16:49:57.854051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Close|\n",
      "+-----+\n",
      "| 60.3|\n",
      "| 59.7|\n",
      "| 59.4|\n",
      "| 59.0|\n",
      "| 59.2|\n",
      "| 59.0|\n",
      "| 59.4|\n",
      "| 59.5|\n",
      "| 59.5|\n",
      "| 59.8|\n",
      "| 60.0|\n",
      "| 60.6|\n",
      "| 61.0|\n",
      "| 60.9|\n",
      "| 61.4|\n",
      "| 61.5|\n",
      "| 61.0|\n",
      "| 60.7|\n",
      "| 61.3|\n",
      "| 61.4|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "df.select(format_number('Close', 1).alias('Close')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19. (1p) Which is the month with the highest Open value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-03T16:51:21.565570Z",
     "end_time": "2023-05-03T16:51:21.861117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|Month|        max(Open)|\n",
      "+-----+-----------------+\n",
      "|    1|        90.800003|\n",
      "|    2|        87.260002|\n",
      "|   12|87.08000200000001|\n",
      "|   11|            86.18|\n",
      "|    3|            83.93|\n",
      "|    4|        82.279999|\n",
      "|    5|        79.730003|\n",
      "|    7|            78.68|\n",
      "|    8|        78.620003|\n",
      "|   10|        78.110001|\n",
      "|    6|        77.459999|\n",
      "|    9|        77.129997|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn('Month', month(df['Date']))\n",
    "df2.groupBy('Month').max('Open').orderBy('max(Open)', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20. (1p) Which is the day by month with a higher difference between the High and the Low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-03T16:56:45.239422Z",
     "end_time": "2023-05-03T16:56:46.000965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|Day|         max(Diff)|\n",
      "+---+------------------+\n",
      "| 27|1.3799969999999888|\n",
      "| 23|1.4000010000000032|\n",
      "| 31|  1.58000100000001|\n",
      "|  5|1.6500020000000006|\n",
      "|  4|              1.75|\n",
      "| 12|          1.760002|\n",
      "| 16|1.8199999999999932|\n",
      "| 21|1.8800010000000071|\n",
      "| 11| 1.889999000000003|\n",
      "| 26|1.8999939999999924|\n",
      "|  2|1.9400029999999902|\n",
      "| 22|2.0299990000000037|\n",
      "|  1|2.0400010000000037|\n",
      "| 20|2.1499939999999924|\n",
      "|  8| 2.150000999999989|\n",
      "| 10|  2.16999899999999|\n",
      "| 28|2.1899939999999987|\n",
      "| 13| 2.199996999999996|\n",
      "|  9|2.2099989999999963|\n",
      "|  7|2.3100049999999968|\n",
      "+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dayofmonth\n",
    "df2 = df.withColumn('Day', dayofmonth(df['Date']))\n",
    "df2 = df2.withColumn('Diff', df2['High'] - df2['Low'])\n",
    "df2.groupBy('Day').max('Diff').orderBy('max(Diff)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 21. (0.5p) Compute the median of the Close column\n",
    "Note: You will need to explore a bit how to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-03T16:57:18.044033Z",
     "end_time": "2023-05-03T16:57:18.199436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Close)|\n",
      "+-----------------+\n",
      "|72.38844998012726|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(mean('Close')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 22. (1p) Compute the difference in days between the max and the min Date\n",
    "Note: You will need to explore a bit how to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-03T16:58:13.788293Z",
     "end_time": "2023-05-03T16:58:14.201239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|datediff(max(Date), min(Date))|\n",
      "+------------------------------+\n",
      "|                          1823|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, max, min\n",
    "df.select(datediff(max('Date'), min('Date'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
